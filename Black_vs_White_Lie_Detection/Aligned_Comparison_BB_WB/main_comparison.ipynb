{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed5ddaf6",
   "metadata": {},
   "source": [
    "# Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81668353",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from load_data import load_commonsense_qa\n",
    "from wb_activations import load_model, get_activations_for_dataset\n",
    "from bb_logprobs import compute_bb_features\n",
    "from wb_probes import train_wb_probes\n",
    "from bb_classifier import train_bb_classifier\n",
    "from evaluation_utils import compare_results\n",
    "\n",
    "# Settings\n",
    "# You can change this to a lighter model like 'gpt2' for debugging if needed, \n",
    "# but Llama-2-7b-chat is recommended for meaningful lie detection.\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\" \n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eb0ab1",
   "metadata": {},
   "source": [
    "# 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f34571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the validation split for quick experimentation.\n",
    "df = load_commonsense_qa(split='validation')\n",
    "print(f\"Loaded dataset keys: {df.columns}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efea3d6f",
   "metadata": {},
   "source": [
    "# 2. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2584c106",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure you have access to the model weights.\n",
    "model, tokenizer = load_model(MODEL_NAME, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efafaee8",
   "metadata": {},
   "source": [
    "# 3. White Box: Extract Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49230aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We extract activations for layers 10, 15, 20 (mid-to-late layers usually have the best representation)\n",
    "# Limit to 50 samples for testing the pipeline flow. Remove .head(50) for full run.\n",
    "sample_df = df.head(50) \n",
    "print(f\"Processing {len(sample_df)} samples...\")\n",
    "\n",
    "wb_data = get_activations_for_dataset(sample_df, model, tokenizer, device=DEVICE) \n",
    "print(f\"Extracted activations for {len(wb_data)} scenarios (True + False).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b79956",
   "metadata": {},
   "source": [
    "# 4. Black Box: Compute Logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa65dc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We assume the SAME history context, but instead of activations, we query probing questions.\n",
    "X_bb, y_bb = compute_bb_features(sample_df, model, tokenizer, device=DEVICE)\n",
    "print(f\"Computed Black Box features with shape: {X_bb.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e246d6",
   "metadata": {},
   "source": [
    "# 5. Train & Evaluate Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed0c009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# White Box Probes\n",
    "# Train Logistic Regression probes on the extracted layer activations\n",
    "wb_probes, wb_metrics = train_wb_probes(wb_data, layer_list=[10, 15, 20])\n",
    "\n",
    "# Black Box Classifier\n",
    "# Train Logistic Regression on the logprob features\n",
    "bb_model, bb_metric = train_bb_classifier(X_bb, y_bb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4c1ff6",
   "metadata": {},
   "source": [
    "# 6. Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0c7824",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = compare_results(wb_metrics, bb_metric)\n",
    "comparison_df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
