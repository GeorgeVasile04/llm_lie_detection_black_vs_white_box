{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b11942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Import Utilities\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dotenv\n",
    "import os\n",
    "import openai\n",
    "\n",
    "# Load environment variables (API keys)\n",
    "dotenv.load_dotenv()\n",
    "# openai.api_base = \"https://api.deepinfra.com/v1/openai\" # Uncomment and set if using DeepInfra/Llama 2\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Import our custom utilities\n",
    "import black_box_utils as utils\n",
    "\n",
    "# Configure pandas display\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Define the model we are prepping for\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\" \n",
    "# MODEL_NAME = \"text-davinci-003\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2a478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the Dataset\n",
    "# We use the custom loader for CommonSense2 (binary questions)\n",
    "dataset = utils.load_dataset()\n",
    "print(f\"Loaded {len(dataset)} questions from {dataset.default_processed_filename}\")\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1773f85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Check Model Capability (Can Answer?)\n",
    "# We need to find N questions that the model answers CORRECTLY.\n",
    "# If the model doesn't know the truth, we can't test if it's lying.\n",
    "\n",
    "NUMBER_qs_TO_PREP = 100 # Adjust this based on how many valid samples you want finally\n",
    "\n",
    "print(f\"Checking if {MODEL_NAME} can answer correctly...\")\n",
    "dataset.check_if_model_can_answer(\n",
    "    model=MODEL_NAME,\n",
    "    max_questions_to_try=NUMBER_qs_TO_PREP,\n",
    "    max_batch_size=10, # Lower batch size for local/smaller APIs\n",
    "    save_progress=True,\n",
    "    bypass_cost_check=True\n",
    ")\n",
    "\n",
    "# Show stats\n",
    "answered_correctly = dataset[f\"{MODEL_NAME}_can_answer\"].sum()\n",
    "attempted = dataset[f\"{MODEL_NAME}_can_answer\"].count()\n",
    "print(f\"Model answered correctly: {answered_correctly}/{attempted}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edbe3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Check Lie Capability & Double Down Rate\n",
    "# We need to verify that our lie instructions actually work for this model.\n",
    "# This step also calculates the 'double down rate' (consistency in lying).\n",
    "\n",
    "print(f\"Testing lie instructions on {MODEL_NAME}...\")\n",
    "\n",
    "# We calculate proper number of rows to try based on successful answers\n",
    "valid_rows = np.where(dataset[f\"{MODEL_NAME}_can_answer\"] == True)[0]\n",
    "n_rows_to_test = len(valid_rows)\n",
    "\n",
    "if n_rows_to_test > 0:\n",
    "    dataset.does_model_lie(\n",
    "        model=MODEL_NAME,\n",
    "        max_questions_to_try=min(50, n_rows_to_test), # Test on a subset of 50 valid questions\n",
    "        lie_instructions=dataset.lie_instructions, # Test all configured instructions\n",
    "        save_progress=True,\n",
    "        bypass_cost_check=True\n",
    "    )\n",
    "\n",
    "    # Compute and display rates\n",
    "    lying_ratio = dataset.compute_lying_ratio(model=MODEL_NAME, lie_instructions=dataset.lie_instructions)\n",
    "    dd_rate = dataset.compute_lie_double_down_rate(model=MODEL_NAME, lie_instructions=dataset.lie_instructions)\n",
    "    \n",
    "    print(\"\\nLying Success Rate per Instruction:\")\n",
    "    print(lying_ratio)\n",
    "    print(\"\\nDouble Down Rate per Instruction:\")\n",
    "    print(dd_rate)\n",
    "else:\n",
    "    print(\"No questions answered correctly, skipping lie test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f52ec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Generate False Statements (Preprocessing for Consistency)\n",
    "# For non-binary datasets, we pre-generate the lie text so we can verify it's actually false\n",
    "# For binary (CommonSense2), this function handles the skipping automatically.\n",
    "\n",
    "print(f\"Generating false statements for {MODEL_NAME}...\")\n",
    "utils.generate_lies(dataset, model=MODEL_NAME, number_questions_to_answer=NUMBER_qs_TO_PREP)\n",
    "\n",
    "# Verify the result\n",
    "cols_to_show = [\"question\", \"answer\", \"false_statement\"]\n",
    "if \"false_statement\" in dataset.columns:\n",
    "    dataset[cols_to_show].head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df49382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Final Status Check\n",
    "# Show how many samples are fully ready (Answerable + (Optional) Lie Generated)\n",
    "\n",
    "ready_mask = dataset[f\"{MODEL_NAME}_can_answer\"] == True\n",
    "# If non-binary, also check false_statement\n",
    "if not dataset.binary_questions and \"false_statement\" in dataset.columns:\n",
    "    ready_mask = ready_mask & dataset[\"false_statement\"].notna()\n",
    "\n",
    "n_ready = ready_mask.sum()\n",
    "print(f\"\\nTotal samples ready for detection experiment: {n_ready}\")\n",
    "\n",
    "# The dataset saves automatically to JSON during these calls.\n",
    "# You can now proceed to Black_Box_Lie_Detection.ipynb and skip these steps!\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
