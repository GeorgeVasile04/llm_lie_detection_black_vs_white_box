{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33828de3",
   "metadata": {},
   "source": [
    "# Black box lie detector\n",
    "\n",
    "This notebook implements a black-box lie detector based on the concept of elicitation queries. The methodology typically follows these steps:\n",
    "\n",
    "1.  **Preprocessing**: Verify that the LLM knows the correct answer to a factual question and can be instructed to lie about it.\n",
    "2.  **Lie Generation**: Generate specific false statements for the questions the model can answer.\n",
    "3.  **Data Generation**: Prompt the model to answer questions both truthfully and deceptively.\n",
    "4.  **Elicitation**: After the model answers, ask a series of fixed \"probe\" questions (e.g., \"Are you telling the truth?\", \"Is the previous answer accurate?\").\n",
    "5.  **Feature Extraction**: Calculate the log-probability differences between the tokens \"Yes\" and \"No\" for each probe question.\n",
    "6.  **Classification**: Train a supervised classifier (e.g., Logistic Regression) on these log-probability features to distinguish between truthful and deceptive responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba4fb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Import Utilities\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dotenv\n",
    "import os\n",
    "import openai\n",
    "\n",
    "# Load environment variables (API keys)\n",
    "dotenv.load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Import our custom utilities\n",
    "import black_box_utils as utils\n",
    "\n",
    "# Configure pandas display\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a391e7",
   "metadata": {},
   "source": [
    "# Step 1: Load the Dataset and the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8271ffe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (We load this early to have the object ready for preprocessing steps)\n",
    "# This now loads the 'CommonSense2' dataset (common_sens_qa_v2.json) for fair comparison with White Box\n",
    "dataset = utils.load_dataset()\n",
    "dataset.head() \n",
    "\n",
    "# Define the model we want to use (Llama-2-7b-chat-hf)\n",
    "# You can use providers like DeepInfra, Together AI, or Anyscale that support OpenAI-compatible APIs\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\" \n",
    "\n",
    "# Configure API for Llama 2 (Example using DeepInfra - Replace with your provider's details)\n",
    "# openai.api_base = \"https://api.deepinfra.com/v1/openai\" \n",
    "# openai.api_key = \"YOUR_DEEPINFRA_API_KEY\" \n",
    "# Ensure your environment variables are set if you prefer that method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015ec6e2",
   "metadata": {},
   "source": [
    "# Step 2: Preprocessing - Test Knowledge and Lie Capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359cc40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We check if the model answers correctly and if it follows instructions to lie.\n",
    "NUMBER_OF_QUESTIONS = 40\n",
    "\n",
    "utils.check_model_capabilities(dataset, model=MODEL_NAME, number_questions_to_answer=NUMBER_OF_QUESTIONS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190235a3",
   "metadata": {},
   "source": [
    "# Step 3: Generate the Lies\n",
    "For binary questions (True/False), explicit lie generation is often skipped because the lie is simply the negation of the truth. Our utility handles this automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b1e0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We generate specific false statements (named lies) for the questions.\n",
    "utils.generate_lies(dataset, model=MODEL_NAME, number_questions_to_answer=NUMBER_OF_QUESTIONS)\n",
    "\n",
    "# Inspect a sample\n",
    "dataset[[\"question\", \"answer\", \"false_statement\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a73527",
   "metadata": {},
   "source": [
    "# Step 4: Ask Questions and Elicitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76378ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We ask the model to answer (truthfully or deceptively) and then ask 48 elicitation questions (probes).\n",
    "# We compute the log-probability differences (Yes - No) for these probes.\n",
    "\n",
    "utils.generate_dataset_with_logprobs(dataset, model=MODEL_NAME, number_questions_to_answer=NUMBER_OF_QUESTIONS)\n",
    "\n",
    "# Check the generated data\n",
    "dataset[[f\"{MODEL_NAME}_logprobs_difference_truth\", f\"{MODEL_NAME}_logprobs_difference_lie\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3452ca",
   "metadata": {},
   "source": [
    "# Step 5: Train the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4560415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We prepare the data (split into train/test) and train a Logistic Regression classifier on the logprobs.\n",
    "\n",
    "X_train, X_test, y_train, y_test = utils.prepare_classification_data(dataset, model=MODEL_NAME)\n",
    "classifier, accuracy, auc, conf_matrix = utils.train_and_evaluate_classifier(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d858bfe",
   "metadata": {},
   "source": [
    "# Step 6: Visualize and Interpret the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af920e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the prediction probabilities distribution for Truth vs Lie classes.\n",
    "utils.visualize_results(classifier, X_test, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
