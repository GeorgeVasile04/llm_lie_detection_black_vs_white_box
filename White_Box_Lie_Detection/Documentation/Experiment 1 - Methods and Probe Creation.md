# Experiment 1: Methods & Probe Creation

## Goal
The objective of this foundational phase is to build the "machinery" required for White Box Lie Detection. Unlike the Black Box approach which analyzes the text generated by the model, this phase focuses on **Representation Engineering**: establishing a pipeline to extract the internal mental state of the LLM and mathematically distill it into a "Lie Detector".

**The specific goals are:**
1.  **Extraction:** reliably accessing the model's hidden thoughts (activations).
2.  **Creation:** training mathematical Probes (vectors) that define which direction in that thought-space represents "Truth".

We are not yet testing if they generalize well (that is Experiment 2). We are simply building the tools.

## Input
1.  **The Subject:** The `Llama-2-13b-chat` Large Language Model.
2.  **The Raw Data:** 18 diverse datasets serves as the training ground.
    *   **DLK (8 datasets):** Binary Q&A tasks (e.g., Sentiment Analysis).
    *   **RepE (5 datasets):** Multiple-choice reasoning tasks.
    *   **GoT (5 datasets):** Pure factual statements (e.g., "Paris is in France").

## Basics of the Code
This section details the specific file structures and functions that power the data handling and activation extraction.


### 1. Where are the Datasets? (The "Just-in-Time" System)
While the project structure contains a `repeng/datasets/data` folder, it does not hold the raw GBs of source files. Instead, the project uses a **programmatic download system**.

*   **The Mechanism:** All 18+ datasets are handled by the script files in `repeng/datasets/elk/` (e.g., `arc.py`, `truthful_qa.py`, `dlk.py`). These scripts use the **Hugging Face `datasets` library**.
    *   **First Run:** When the code calls `load_dataset`, it downloads the raw data from the Hugging Face Hub to your local cache (usually `~/.cache/huggingface`).
    *   **Subsequent Runs:** It loads instantly from this local cache, ensuring efficiency and reproducibility.
*   **The Uniform Format (`BinaryRow`):** 
    Since every raw dataset has a different structure (different column names like "question", "sentence", "text"), these scripts act as adapters. They convert every entry into a standardized **`BinaryRow` object** (defined in `repeng/datasets/elk/types.py`).
    *   **Structure:** A `BinaryRow` guarantees the rest of the pipeline always sees:
        *   `text`: The formatted prompt fed to the model.
        *   `label`: The boolean Truth/False value.
        *   `dataset_id`: The source name.
        *   `group_id`: (Optional) Used to link related rows (e.g., two answers to the same question).
*   **Definitions:** `repeng/datasets/elk/types.py` lists the valid `DatasetId`s.
>>>>>>> de1d9388750f5b71140c7866df78dd3a708deab4
*   **Access:** The function `get_dataset` in `repeng/datasets/elk/utils/fns.py` acts as a router, calling the specific loader function for the requested ID.

### 2. The Activation Pipeline (`create_activations_dataset`)
The function `create_activations_dataset` (in `repeng/datasets/activations/creation.py`) is the high-level manager. It does not run the model itself but orchestrates the workflow:
1.  **Iterates:** It loops through the requested models (e.g., Llama-2-13b) and datasets.
2.  **Filters:** It applies `group_limits` to ensure we don't process more data than needed (e.g., only 1000 examples).
3.  **Output:** It produces a list of `ActivationResultRow` objects. Each row corresponds to one example (text) and contains:
    *   `dataset_id`: Where it came from.
    *   `label`: The ground truth (True/False).
    *   `activations`: A dictionary mapping layer names to their vector values.

### 3. Deep Dive: Extraction Mechanics (`get_model_activations`)
The low-level work is performed by `get_model_activations` in `repeng/activations/inference.py`. This is where the code interacts with the neural network.

**The Workflow:**
1.  **Tokenization:** The text string is converted into token IDs (integers) and moved to the GPU (`device="cuda"`).
2.  **Hook Registration:** The code uses `grab_many` to attach a "hook" function to every requested layer in the PyTorch model. These hooks are passive listeners—they wait for data to pass through.
3.  **Forward Pass:** `llm.model.forward(tokens)` is called. As the input flows through the network, the hooks capture a copy of the hidden states at each layer.
4.  **Slicing (Last Token):** The raw activations usually have the shape `(Sequence_Length, Hidden_Dimension)`. Since we typically set `last_n_tokens=1`, the code slices this array to keep only the vector corresponding to the final token. This vector represents the model's final "thought" before predicting the next word.
5.  **Output:** The tensors are detached from the computation graph, converted to NumPy arrays (often float16 to save space), and packaged into an `ActivationRow`.
<<<<<<< HEAD
 
=======

>>>>>>> de1d9388750f5b71140c7866df78dd3a708deab4
## The Process

### 1. Data Formatting & Grouping
Before feeding data to the model, it is standardized.
*   **Grouping:** Crucially, we group related inputs (e.g., the True answer and the False answer for the *same* question). This allows algorithms like **PCA-G** or **CCS** to later subtract the "Topic" (noise) and isolate the "Truth" (signal).

**Code Reference:**
*   **Definition:** `repeng/datasets/elk/types.py` defines the `BinaryRow` structure and the `GroupedDatasetId` list, which determines if a dataset supports grouping.
*   **Logic:** The function `create_activations_dataset` in `repeng/datasets/activations/creation.py` orchestrates the data loading and grouping limits.
*   **Loaders:** Individual dataset loaders (e.g., for DLK or Arc) are located in `repeng/datasets/elk/`.

### 2. Activation Extraction (The "Surgery")
This is the technical core of the experiment.
*   **Feeding:** We run the formatted text through Llama-2.
*   **Hooking:** We use software hooks to intercept the data flowing through the neural network.
*   **Snapshot:** We capture the **Activation Vector ($a$)**—a list of 5,120 numbers representing the model's state.
*   **Location:** We extract this vector from the **Last Token** of the prompt, as this is where the model has processed the full context and formed its conclusion.
*   **Scope:** We extract this for every layer (0-40) to investigate where the concept of "Truth" emerges.

**Code Reference:**
*   **The Hook:** `repeng/hooks/grab.py` contains the `grab` context manager, which registers a PyTorch forward hook to extract the hidden states (`tensor_extractor.extract`).
*   **Inference:** `repeng/activations/inference.py` (via `get_model_activations`) handles the forward pass of the model and extracting activations at specific token positions (`last_n_tokens`).

### 3. Probe Training (The Algorithms)
We feed these "piles" of activation vectors into our 8 algorithms to calculate the Probe Vector ($v$).
*   **Supervised Training:** We tell algorithms like **Difference-in-Means (DIM)** which vectors are True and which are False. They calculate the difference.
*   **Unsupervised Training:** We give algorithms like **LAT** or **PCA** the raw/grouped vectors. They analyze the geometry (variance, spread) to find the most significant direction (which we hypothesize is Truth).

**Code Reference:**
*   **Pipeline & Orchestration:** The training loop is defined in `experiments/comparison.py`. The function `run_pipeline` iterates over all datasets and methods, calling `train_probe` in `repeng/probes/collections.py` to dispatch the correct training function.
*   **Implementations:** All probe algorithms are located in `repeng/probes/`.
    *   **DIM:** `repeng/probes/difference_in_means.py`
    *   **CCS:** `repeng/probes/contrast_consistent_search.py`
    *   **LAT:** `repeng/probes/linear_artificial_tomography.py`
    *   **PCA/PCA-G:** `repeng/probes/principal_component_analysis.py`
    *   **LDA:** `repeng/probes/linear_discriminant_analysis.py`
    *   **LR/LR-G:** `repeng/probes/logistic_regression.py`
*   **Interface:** `repeng/probes/base.py` defines the `Probe` abstract base class that all algorithms implement.

## Output
The result of this experiment is a library of **Trained Linear Probes**.

Mathematically, the output is simply a collection of **Vectors ($v$)**, each of size 5,120.
We generate one vector for every combination of:
*   **Dataset** (e.g., trained on `dbpedia`)
*   **Algorithm** (e.g., trained using `DIM`)
*   **Layer** (e.g., extracted from `Layer 21`)

These vectors are the actual "Lie Detectors" that will be tested in Experiment 2.

### Storage & Execution Note
The results are designed to be stored in `output/comparison` via the `mppr` caching system.
*   **The Artifacts:**
    *   **Probes:** Saved as `.pickle` files (cache key: `probe_train-v2`).
    *   **Scores:** Saved as data rows (cache key: `probe_evaluate-v2`).
*   **Current Status:** The `output/` folder is currently **missing** from the repository. This means there are no pre-trained probes available locally.
*   **Action Required:** To generate these results, `experiments/comparison.py` must be executed. This script will download the activation data and retrain all probes from scratch.