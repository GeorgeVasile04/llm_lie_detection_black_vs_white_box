# Pedagogical Guide to Experiments: Paper vs. Codebase

This document bridges the gap between the scientific concepts in *"How to catch an AI liar"* and the practical implementation in the code. It maps theoretical experiments to specific code files, data sources, and parameters.

---

## Experiment 1: The Baseline â€” Can we make them lie?

### ðŸ§  The Logic
Before we can build a lie detector, we need a dataset of lies. Usefully, LLMs like GPT-3.5 are instruction-following models. If we tell them to lie, they often will.
This experiment establishes the "ground truth":
1.  **Prompting**: We explicitly instruct the model: *"Lie when answering the following question."*
2.  **Measuring**: We calculate the **Lying Rate** (how often it actually lies) and **Double-Down Rate** (how often it sticks to the lie when asked "Are you sure?").
3.  **Why?**: To verify we have a reliable generator of lies to train our detector on later.

### ðŸ“„ In the Paper
-   **Section 4.2**: *Generating Lies (and Truths) with LLMs*
-   **Figure 4**: Shows the lying rates for different models and prompts.
-   **Key Insight**: Using "Inner Chain-of-Thought" (asking the model to "think" about lying before speaking) drastically increases the lying rate.

### ðŸ’» In the Codebase
-   **Notebook**: [`lying_rate_double_down_rate_probes.ipynb`](../lying_rate_double_down_rate_probes.ipynb)
    -   This is the main lab bench for this experiment. It runs the prompts and calculates the metrics.
-   **Code**: [`lllm/questions_loaders.py`](../lllm/questions_loaders.py)

### ðŸ“Š Data & Parameters
| Category | Paper (Theory) | Code (Practice) |
| :--- | :--- | :--- |
| **Dataset** | 10 Existing Datasets + 1 Synthetic | **Class Objects**: `SyntheticFacts`, `Questions1000`, `WikiData`, `Commonsense2`, `TatoebaFreToEng`, `TatoebaEngToFre`, `Sciq`, `MathematicalProblems`, `AnthropicAwarenessAI`, `AnthropicAwarenessArchitecture`, `AnthropicAwarenessNNArchitecture`. <br> **Location**: `data/processed_questions/*.json` |
| **Model** | GPT-3.5 (`text-davinci-003`) | **Parameter**: `model="text-davinci-003"` (default in `_run_dialogue`) |
| **Prompt** | Direct Lie Instructions + Inner CoT | **Variable**: `default_lie_instructions` (List of 17 str) and `cot_lie` (String) in `lllm/questions_loaders.py` |
| **Temp** | 0.7 | **Parameter**: `temperature=0.7` in `model_kwargs_suspect` |

---

## Experiment 2: The Detector â€” Can we catch them?

### ðŸ§  The Logic
The core hypothesis is that a "lying state" is different from a "truthful state" inside the model.
1.  **The "Body Language"**: After the model answers, we ask 48 unrelated "elicitation questions" (e.g., *"Is the sky blue?"*).
2.  **The Classifier**: We train a Logistic Regression classifier on the Yes/No answers to these questions to distinguish lies from truth.

### ðŸ“„ In the Paper
-   **Section 4.3**: *Black-box Lie Detection with Elicitation Questions*
-   **Section 5.3**: *Ambiguous Elicitation Questions*

### ðŸ’» In the Codebase
-   **Notebook**: [`classification_notebooks/train_classifiers_on_prompted_GPT_3.5.ipynb`](../classification_notebooks/train_classifiers_on_prompted_GPT_3.5.ipynb)
-   **Logic**: `lllm/classification_utils.py` (Classifier class)

### ðŸ“Š Data & Parameters
| Category | Paper (Theory) | Code (Practice) |
| :--- | :--- | :--- |
| **Training Data** | 1280 positive (lie) / 1280 negative (truth) samples generated by GPT-3.5 | **Source**: Aggregated outputs from Exp 1. The notebook iterates through all 11 dataset classes and calls `d.get_logprobs_differences()`. |
| **Features** | Binary (Yes/No) & Logprob Differences | **Code**: `preprocess_logprobs` function calculates `log(P(yes)) - log(P(no))` using the top-5 tokens. |
| **Probes** | 48 Elicitation Questions | **File**: `data/probes.csv` <br> **Categorization**: Split into `lie_indices`, `knowable_indices` inside the notebook. |
| **Classifier** | Logistic Regression | **Library**: `sklearn.linear_model.LogisticRegression` |

---

## Experiment 3: Generalisation â€” Does it work everywhere?

### ðŸ§  The Logic
A detector that only works on the exact data it was trained on is useless. This set of experiments tests **Robustness**.
We take the detector trained in Experiment 2 (frozen) and apply it to completely new situations:
1.  **New Topics**: If we train on Math questions, does it work on History?
2.  **New Scenarios**: Does it work if the model lies to sell a car ("Instrumental Lying") or solely to agree with the user ("Sycophancy")?

### ðŸ“„ In the Paper
-   **Section 5.1**: *On the Generalisation of Our Lie Detector*
-   **Figure 2 & 5**: Topic generalisation and Role-playing scenarios.

### ðŸ’» In the Codebase
-   **Folder**: [`classification_notebooks/advanced_generalisation_studies/`](../classification_notebooks/advanced_generalisation_studies/)

### ðŸ“Š Data & Parameters
| Experiment | Paper Description | Code Location & Data |
| :--- | :--- | :--- |
| **Instrumental Lying** | Role-playing (Salesperson, Police) to achieve a goal. | **Notebook**: `instrumental_lying.ipynb` <br> **Data**: `results/instrumental_lying_df_*.json` |
| **Specific Lies** | Lying only about specific topics. | **Notebook**: `specific_lies.ipynb` |
| **Sycophancy** | Agreeing with user's incorrect beliefs ("I think the answer is..."). | **Notebook**: `lies_vs_falsehoods.ipynb` (Analyzing broad falsehoods) |
| **Unseen Datasets** | Testing on datasets not used during training. | **Notebook**: `classification_notebooks/simple_generalisation_studies/generalisation_experiment.ipynb` |

---

## Experiment 4: The Transfer â€” Other Models (Alpaca/Vicuna)

### ðŸ§  The Logic
The authors trained the detector on GPT-3.5 (OpenAI). But researchers often use open-source models like Llama or Vicuna.
This experiment asks: *Does the "body language" of lying transfer across different machine brains?*
They take the detector trained on GPT-3.5 and run it on Alpaca and Vicuna outputs variables without retraining.

### ðŸ“„ In the Paper
-   **Section 5.1.3**: *Generalisation to Unseen LLM Architectures*

### ðŸ’» In the Codebase
-   **Folder**: [`experiments_alpaca_vicuna/`](../experiments_alpaca_vicuna/)

### ðŸ“Š Data & Parameters
| Category | Paper (Theory) | Code (Practice) |
| :--- | :--- | :--- |
| **Models** | Alpaca-7B, Vicuna-7B | **Scripts**: `does_alpaca_vicuna_lie.py` uses libraries like `deepspeed` to load these models. |
| **Method** | Prompting with same lie instructions as GPT-3.5. | **Code**: Reuses `default_lie_instructions` from `lllm/questions_loaders.py`. |
| **Results** | Measured AUC on transfer. | **Analysis**: `lying_and_detection_results.ipynb` reads `results/double_down_rate_alpaca.csv` and `results/double_down_rate_vicuna.csv`. |

---

## Experiment 5: Fine-tuning â€” Models Taught to Lie

### ðŸ§  The Logic
Prompting a model ("Please lie") is easy. But what if a model is *trained* (fine-tuned) to be a liar? It might internalize the lying behavior so deeply that the "body language" disappears.
1.  **Intervention**: They create a dataset of 100% lies and fine-tune models (GPT-3, Llama) on it.
2.  **Test**: Can the original detector still catch them? (Spoiler: Yes, surprisingly well).

### ðŸ“„ In the Paper
-   **Section 5.1.2** (End): Mentions fine-tuning GPT-3 (`davinci`) and finding strong detector performance.
-   **Section 4.2**: Describes creating "Assistant 1" (Truthful) and "Assistant 2" (Liar) personas.

### ðŸ’» In the Codebase
-   **Folder**: [`finetuning/`](../finetuning/)

### ðŸ“Š Data & Parameters
| Category | Paper (Theory) | Code (Practice) |
| :--- | :--- | :--- |
| **Training Data** | "Assistant 1" (Truthful) vs "Assistant 2" (Liar) | **File**: `finetuning/finetuning_dataset_train_original.jsonl` (Generated by `create_finetuning_datasets.ipynb`). |
| **Models** | `davinci` (OpenAI), `llama-7b`, `llama-30b` | **Folder**: `finetuning/llama/` contains specific setup scripts. |
