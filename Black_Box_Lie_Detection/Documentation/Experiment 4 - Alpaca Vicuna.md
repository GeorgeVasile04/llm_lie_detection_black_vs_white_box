# Experiment 4: Generalisation to Open Source Models (Alpaca & Vicuna)

## 1. Overview
The previous experiments (1, 2, and 3) were all conducted on **GPT-3.5 (`text-davinci-003`)**. We established that we can generate a dataset of lies, train a simple classifier to detect them using "elicitation questions" (probes), and that this detector is robust to different *lie instructions*.

**Experiment 4** addresses the next critical question: **Is this "Lie Detection" mechanism specific to GPT-3.5, or does it generalize to completely different Large Language Models?**

This is a test of **Cross-Model Generalisation**.
*   We take the **exact classifier trained on GPT-3.5**.
*   We apply it to data generated by **Alpaca 7B**, **Vicuna 7B**, and **Vicuna 13B**.
*   We do **not** retrain or fine-tune the detector on these new models.

If the detector works, it implies that "lying" (the internal state of saying something false when knowing the truth) shares a common "signature" or toplogy across different model architectures.

---

## 2. Where It Starts (Input)

### Models
We move from the proprietary API-based GPT-3.5 to open-source models based on LLaMA, which allows for local execution and inspection.
*   **Alpaca 7B**: Stanford's fine-tuned LLaMA model.
*   **Vicuna 7B & 13B**: Chat-optimized models finetuned on ShareGPT conversations.

### Classifiers
The experiment uses the **Models trained in Experiment 2** (stored in `results/trained_classifiers/`).
*   Ideally, a classifier that learned "what a lie looks like" on Davinci-003 should fail on Vicuna-13B if the signal is just random noise.
*   If it succeeds, the "Lie Signal" is universal.

---

## 3. The Process (Methodology)

Since we are changing the underlying engine, we must re-run the data generation pipeline to verify the baselines before testing detection. This is handled by scripts in the `experiments_alpaca_vicuna/` folder.

### Step 1: Capability Check
*   **Script:** `can_alpaca_vicuna_answer.py`
*   **Goal:** Before we can check if they lie, we must verify they know the truth.
*   **Method:** We ask the standard questions (from WikiData, Commonsense2, etc.) and check accuracy.
*   **Observation:** These smaller models (7B/13B) are generally less knowledgeable than GPT-3.5, filtering out more questions in the "Can Answer" phase.

### Step 2: Lying Behavior Analysis
*   **Script:** `does_alpaca_vicuna_lie.py`
*   **Goal:** Can these models even be instructed to lie?
*   **Method:** We prompt them with "Answer with a lie..." and check if the output is false.
*   **Observation:**
    *   **"Bad Liars":** Alpaca and Vicuna are often "too honest" or simply confused by the instruction. The **Lying Rate** is significantly lower than GPT-3.5 (e.g., ~15-20% on some datasets where GPT-3.5 was ~60%).
    *   **Refusals:** They frequently output "I cannot answer that" or valid answers despite instructions.

### Step 3: Double-Down Consistency
*   **Script:** `lying_and_detection_results.ipynb`
*   **Goal:** When they *do* lie, do they stick to it?
*   **Method:** We calculated the **Double-Down Rate** (consistency of the lie).
*   **Observation:** Surprisingly high (often >80%). If you force them to lie, they commit to the bit, similar to GPT-3.5.

### Step 4: Cross-Model Detection (The Core Experiment)
*   **Script:** `generate_alpaca_vicuna_logprobs.py` & `lying_and_detection_results.ipynb`
*   **Method:**
    1.  Generate the "probes" (yes/no elicitation questions) on Alpaca/Vicuna while they are lying/telling the truth.
    2.  Extract the `logprobs` differences ($P(Yes) - P(No)$).
    3.  Feed these vectors into the **GPT-3.5-trained Logistic Regression**.
    4.  Measure AUC (Area Under the Curve).

---

## 4. The Results (Output)

The results are analyzed in `lying_and_detection_results.ipynb`.

### 1. Zero-Shot Transfer Works
The detector achieves **above-random performance** (and often high accuracy) on these completely unseen models.
*   **Implication:** The statistical "fingerprint" of a lie (e.g., a drop in confidence on unrelated factual questions like "Is the sky blue?") is consistent across models.
*   **Vicuna vs Alpaca:** Vicuna generally shows better transfer results than Alpaca, possibly because its instruction-tuning data (ShareGPT) is closer to the RLHF nature of GPT-3.5 than Alpaca's synthetic dataset.

### 2. Generalisation across Architectures
This confirms the main hypothesis of the thesis: The method extracts a **structure of truthfulness** rather than overfitting to specific model weights or token patterns. The "lie detector" is effectively a "state detector" that works across the LLaMA family.

### 3. Limitations
*   **Lower Performance:** While it works, the accuracy is lower than on the source domain (Training on GPT-3.5 -> Testing on GPT-3.5).
*   **Data Scarcity:** Because Alpaca/Vicuna are worse at following lying instructions, the test sets are smaller (fewer successful lies to test against).
