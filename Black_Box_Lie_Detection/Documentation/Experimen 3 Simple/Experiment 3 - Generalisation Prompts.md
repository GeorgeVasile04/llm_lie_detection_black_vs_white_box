# Experiment 3: Generalisation Prompts

## 1. Overview
This experiment answers a crucial question: **Does the detector fundamentally understand lying, or is it just memorizing the "command" to lie?**

In Experiment 1, we used specific prompts to generate lies (e.g., *"You are a deceptive assistant"*). There is a risk that the classifier simply learned to associate the specific wording of that prompt with the label "Lie". This experiment tests **Instruction Robustness** by checking if the detector works when the model is given a *command to lie* that the detector has never seen before.

---

## 2. Where It Starts (Input)
*   **Notebook:** `classification_notebooks/simple_generalisation_studies/prompts_generalization_experiments.ipynb`
*   **Data:** A dataset containing logprobs generated using various instructions.
*   **Variables:**
    *   `lie_instructions_list`: A list of different strings used to prompt the model to lie.
    *   `truth_instructions_list`: A list of different strings used to prompt the model to tell the truth.

---

## 3. The Process (Methodology)

The experiment uses a specific variation of **Leave-One-Out (LOO)** cross-validation, but applied to **Instructions** rather than datasets.

### Step 1: Pair Selection
The code iterates through every possible combination of:
*   **1 Held-Out Lie Instruction** (e.g., *"Answer wrong"*).
*   **1 Held-Out Truth Instruction** (e.g., *"Answer correctly"*).

These two specific prompts form the **"Unseen Test Set"**.

### Step 2: Training (The "Known" World)
*   The detector is trained on **ALL data excluding** the rows generated by the held-out pair.
*   This means the classifier learns from 16 different ways of lying, but not the 17th specific way we are about to test.

### Step 3: Testing (The "Alien" Command)
*   The trained model is then evaluated on the data generated by the held-out pair.
*   **Metric:** We check if the calibrated "Lie Signal" appears even though the prompt causing the lie is completely novel to the classifier.

---

## 4. The Results (Output)

### 1. High Generalisation
The resulting heatmaps (AUC matrices) show that performance remains extremely high (often >99%) even on held-out instructions.
*   **Interpretation:** The "internal state of lying" is consistent regardless of *how* the model was told to lie. Whether you say *"You are a liar"* or *"Please be deceptive"*, the model's internal probability shifts on the probes (elicitation questions) remain the same.

### 2. The Artifacts
*   **Visualizations:** The notebook produces heatmaps comparing "In-Distribution" (Reference) vs "Out-of-Distribution" (Held-out) performance.
*   **Key Finding:** The relative decrease in performance is negligible (mostly blue/white on the difference map), proving the detector is not overfitting to the specific prompt text.
